https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/
----------------------
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2, interaction_only=False)
X_poly = poly.fit_transform(X)
X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(X_poly, y, test_size=0.2, random_state=1)
X_train_poly.shape

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_poly_scaled = pd.DataFrame(scaler.fit_transform(X_train_poly))
X_test_poly_scaled = pd.DataFrame(scaler.transform(X_test_poly))

model2 = Sequential()

model2.add(Dense(64, input_shape=(105,), activation='relu'))
model2.add(Dense(32, activation='tanh'))
model2.add(Dense(1, activation='sigmoid'))

sgd = optimizers.Adam(lr=0.001)

model2.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])
model2.fit(X_train_poly_scaled, y_train_poly, epochs=350, batch_size=800, verbose=1)

result2 = model2.evaluate(X_test_poly_scaled, y_test_poly.values)
----------------------
Sigmoid activate function:
	- Not 0-centered
	- Vanishing gradient problem => Trains too long fore earlier layers
RELU
	- Dying RELU
----------------------
https://towardsdatascience.com/understanding-the-mathematics-behind-gradient-descent-dde5dc9be06e
