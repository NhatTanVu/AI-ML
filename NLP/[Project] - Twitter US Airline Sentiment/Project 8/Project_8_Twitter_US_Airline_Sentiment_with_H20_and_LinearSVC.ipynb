{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context\n",
    "## Description\n",
    "A sentiment analysis job about the problems of each major U.S. airline. <br/>\n",
    "Twitter data was scraped from February of 2015 and contributors were asked to:\n",
    "- Classify positive, negative, and neutral tweets.\n",
    "- Categorizing negative reasons (such as \"late flight\" or \"rude service\").\n",
    "\n",
    "## Dataset\n",
    "The dataset has to be downloaded from: https://www.kaggle.com/crowdflower/twitter-airline-sentiment<br/>\n",
    "* tweet_id\n",
    "* airline_sentiment\n",
    "* airline_sentiment_confidence\n",
    "* negativereason\n",
    "* negativereason_confidence\n",
    "* airline\n",
    "* airline_sentiment_gold\n",
    "* name\n",
    "* negativereason_gold\n",
    "* retweet_count\n",
    "* text\n",
    "* tweet_coord\n",
    "* tweet_created\n",
    "* tweet_location\n",
    "* user_timezone\n",
    "\n",
    "# Objective\n",
    "To implement the techniques learnt as a part of the course.\n",
    "\n",
    "# Learning Outcomes\n",
    "- Basic understanding of text pre-processing.\n",
    "- What to do after text pre-processing:\n",
    "    - Bag of words\n",
    "    - Tf-idf\n",
    "- Build the classification model.\n",
    "- Evaluate the Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import libraries and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_colwidth', 0) # Display full dataframe information (Non-turncated Text column.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsData = pd.read_csv(\"Tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 15 columns):\n",
      "tweet_id                        14640 non-null int64\n",
      "airline_sentiment               14640 non-null object\n",
      "airline_sentiment_confidence    14640 non-null float64\n",
      "negativereason                  9178 non-null object\n",
      "negativereason_confidence       10522 non-null float64\n",
      "airline                         14640 non-null object\n",
      "airline_sentiment_gold          40 non-null object\n",
      "name                            14640 non-null object\n",
      "negativereason_gold             32 non-null object\n",
      "retweet_count                   14640 non-null int64\n",
      "text                            14640 non-null object\n",
      "tweet_coord                     1019 non-null object\n",
      "tweet_created                   14640 non-null object\n",
      "tweet_location                  9907 non-null object\n",
      "user_timezone                   9820 non-null object\n",
      "dtypes: float64(2), int64(2), object(11)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "tweetsData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns have null values:\n",
    "* negativereason\n",
    "* negativereason_confidence\n",
    "* airline_sentiment_gold\n",
    "* negativereason_gold\n",
    "* tweet_coord\n",
    "* tweet_location\n",
    "* user_timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513  neutral           1.0000                         \n",
       "1  570301130888122368  positive          0.3486                         \n",
       "2  570301083672813571  neutral           0.6837                         \n",
       "3  570301031407624196  negative          1.0000                         \n",
       "4  570300817074462722  negative          1.0000                         \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0  NaN           NaN                         Virgin America   \n",
       "1  NaN            0.0000                     Virgin America   \n",
       "2  NaN           NaN                         Virgin America   \n",
       "3  Bad Flight     0.7033                     Virgin America   \n",
       "4  Can't Tell     1.0000                     Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0  NaN                    cairdin     NaN                 0               \n",
       "1  NaN                    jnardino    NaN                 0               \n",
       "2  NaN                    yvonnalynn  NaN                 0               \n",
       "3  NaN                    jnardino    NaN                 0               \n",
       "4  NaN                    jnardino    NaN                 0               \n",
       "\n",
       "                                                                                                                             text  \\\n",
       "0  @VirginAmerica What @dhepburn said.                                                                                              \n",
       "1  @VirginAmerica plus you've added commercials to the experience... tacky.                                                         \n",
       "2  @VirginAmerica I didn't today... Must mean I need to take another trip!                                                          \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse   \n",
       "4  @VirginAmerica and it's a really big bad thing about it                                                                          \n",
       "\n",
       "  tweet_coord              tweet_created tweet_location  \\\n",
       "0  NaN         2015-02-24 11:35:52 -0800  NaN             \n",
       "1  NaN         2015-02-24 11:15:59 -0800  NaN             \n",
       "2  NaN         2015-02-24 11:15:48 -0800  Lets Play       \n",
       "3  NaN         2015-02-24 11:15:36 -0800  NaN             \n",
       "4  NaN         2015-02-24 11:14:45 -0800  NaN             \n",
       "\n",
       "                user_timezone  \n",
       "0  Eastern Time (US & Canada)  \n",
       "1  Pacific Time (US & Canada)  \n",
       "2  Central Time (US & Canada)  \n",
       "3  Pacific Time (US & Canada)  \n",
       "4  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsData.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "United            3822\n",
       "US Airways        2913\n",
       "American          2759\n",
       "Southwest         2420\n",
       "Delta             2222\n",
       "Virgin America    504 \n",
       "Name: airline, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsData[\"airline\"].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsData[\"airline_sentiment\"].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN                            5462\n",
       "Customer Service Issue         2910\n",
       "Late Flight                    1665\n",
       "Can't Tell                     1190\n",
       "Cancelled Flight               847 \n",
       "Lost Luggage                   724 \n",
       "Bad Flight                     580 \n",
       "Flight Booking Problems        529 \n",
       "Flight Attendant Complaints    481 \n",
       "longlines                      178 \n",
       "Damaged Luggage                74  \n",
       "Name: negativereason, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsData[\"negativereason\"].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN         14600\n",
       "negative    32   \n",
       "positive    5    \n",
       "neutral     3    \n",
       "Name: airline_sentiment_gold, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsData[\"airline_sentiment_gold\"].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN                                         14608\n",
       "Customer Service Issue                      12   \n",
       "Late Flight                                 4    \n",
       "Can't Tell                                  3    \n",
       "Cancelled Flight                            3    \n",
       "Cancelled Flight\\nCustomer Service Issue    2    \n",
       "Customer Service Issue\\nLost Luggage        1    \n",
       "Flight Attendant Complaints                 1    \n",
       "Customer Service Issue\\nCan't Tell          1    \n",
       "Bad Flight                                  1    \n",
       "Late Flight\\nLost Luggage                   1    \n",
       "Lost Luggage\\nDamaged Luggage               1    \n",
       "Late Flight\\nFlight Attendant Complaints    1    \n",
       "Late Flight\\nCancelled Flight               1    \n",
       "Name: negativereason_gold, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsData[\"negativereason_gold\"].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Drop all other columns except “text” and “airline_sentiment”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsData_reduced = tweetsData[[\"text\", \"airline_sentiment\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Shape of new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsData_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Print first 5 rows of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                             text  \\\n",
       "0  @VirginAmerica What @dhepburn said.                                                                                              \n",
       "1  @VirginAmerica plus you've added commercials to the experience... tacky.                                                         \n",
       "2  @VirginAmerica I didn't today... Must mean I need to take another trip!                                                          \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse   \n",
       "4  @VirginAmerica and it's a really big bad thing about it                                                                          \n",
       "\n",
       "  airline_sentiment  \n",
       "0  neutral           \n",
       "1  positive          \n",
       "2  neutral           \n",
       "3  negative          \n",
       "4  negative          "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsData_reduced.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Text pre-processing: HTML tag removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                         text  \\\n",
       "0  @VirginAmerica What @dhepburn said.                                                                                          \n",
       "1  @VirginAmerica plus you've added commercials to the experience... tacky.                                                     \n",
       "2  @VirginAmerica I didn't today... Must mean I need to take another trip!                                                      \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces & they have little recourse   \n",
       "4  @VirginAmerica and it's a really big bad thing about it                                                                      \n",
       "\n",
       "  airline_sentiment  \n",
       "0  neutral           \n",
       "1  positive          \n",
       "2  neutral           \n",
       "3  negative          \n",
       "4  negative          "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup                           # Import BeautifulSoup.\n",
    "\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "tweetsData_reduced['text'] = tweetsData_reduced['text'].apply(lambda x: strip_html(x))\n",
    "tweetsData_reduced.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On line 3, \"$&amp;$\" was converted back to \"&\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Text pre-processing: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[@, VirginAmerica, What, @, dhepburn, said, .]</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[@, VirginAmerica, plus, you, 've, added, commercials, to, the, experience, ..., tacky, .]</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[@, VirginAmerica, I, did, n't, today, ..., Must, mean, I, need, to, take, another, trip, !]</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[@, VirginAmerica, it, 's, really, aggressive, to, blast, obnoxious, ``, entertainment, '', in, your, guests, ', faces, &amp;, they, have, little, recourse]</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[@, VirginAmerica, and, it, 's, a, really, big, bad, thing, about, it]</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                       text  \\\n",
       "0  [@, VirginAmerica, What, @, dhepburn, said, .]                                                                                                             \n",
       "1  [@, VirginAmerica, plus, you, 've, added, commercials, to, the, experience, ..., tacky, .]                                                                 \n",
       "2  [@, VirginAmerica, I, did, n't, today, ..., Must, mean, I, need, to, take, another, trip, !]                                                               \n",
       "3  [@, VirginAmerica, it, 's, really, aggressive, to, blast, obnoxious, ``, entertainment, '', in, your, guests, ', faces, &, they, have, little, recourse]   \n",
       "4  [@, VirginAmerica, and, it, 's, a, really, big, bad, thing, about, it]                                                                                     \n",
       "\n",
       "  airline_sentiment  \n",
       "0  neutral           \n",
       "1  positive          \n",
       "2  neutral           \n",
       "3  negative          \n",
       "4  negative          "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize  # Import Tokenizer.\n",
    "\n",
    "tweetsData_reduced['text'] = tweetsData_reduced.apply(lambda row: word_tokenize(row['text']), axis=1) # Tokenization of data\n",
    "\n",
    "tweetsData_reduced.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Text pre-processing: Remove the numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_numbers(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'\\d+', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Text pre-processing: Removal of Special Characters and Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords                       # Import stopwords.\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "customlist = ['not', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn',\n",
    "        \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\n",
    "        \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n",
    "        \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "# Set custom stop-word's list as not, couldn't etc. words matter in Sentiment, so not removing them from original data.\n",
    "\n",
    "stopwords = list(set(stopwords) - set(customlist))  \n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            new_words.append(word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Text pre-processing: Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Text pre-processing: Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer         # Import Lemmatizer.\n",
    "import nltk\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "def lemmatize_list(words):\n",
    "    new_words = []\n",
    "    \n",
    "    for word, pos in nltk.pos_tag(words):\n",
    "        if word != '':\n",
    "            new_words.append(lemmatizer.lemmatize(word, get_wordnet_pos(pos)))\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Text pre-processing: Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica what dhepburn say</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>virginamerica plus added commercial experience tacky</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>virginamerica i nt today must mean i need take another trip</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>virginamerica really aggressive blast obnoxious entertainment guest face little recourse</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>virginamerica really big bad thing</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                       text  \\\n",
       "0  virginamerica what dhepburn say                                                            \n",
       "1  virginamerica plus added commercial experience tacky                                       \n",
       "2  virginamerica i nt today must mean i need take another trip                                \n",
       "3  virginamerica really aggressive blast obnoxious entertainment guest face little recourse   \n",
       "4  virginamerica really big bad thing                                                         \n",
       "\n",
       "  airline_sentiment  \n",
       "0  neutral           \n",
       "1  positive          \n",
       "2  neutral           \n",
       "3  negative          \n",
       "4  negative          "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(words):\n",
    "    words = remove_numbers(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = lemmatize_list(words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "tweetsData_reduced['text'] = tweetsData_reduced.apply(lambda row: normalize(row['text']), axis=1)\n",
    "tweetsData_reduced.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Vectorization - Use CountVectorizer + 1000 most-frequently used features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 1000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorization (Convert text data to numbers).\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "vectorizer = CountVectorizer()                # Keep only 1000 features as number of features will increase the processing time.\n",
    "tweetsData_reduced_features_1 = vectorizer.fit_transform(tweetsData_reduced['text'])\n",
    "svd1 = TruncatedSVD(n_components=1000, random_state=1)\n",
    "tweetsData_reduced_features_1 = svd1.fit_transform(tweetsData_reduced_features_1) \n",
    "tweetsData_reduced_features_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Vectorization - Use TfidfVectorizer + 1000 most-frequently used features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 1000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using TfidfVectorizer to convert text data to numbers.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tweetsData_reduced_features_2 = vectorizer.fit_transform(tweetsData_reduced['text'])\n",
    "svd2 = TruncatedSVD(n_components=1000, random_state=1)\n",
    "tweetsData_reduced_features_2 = svd2.fit_transform(tweetsData_reduced_features_2) \n",
    "tweetsData_reduced_features_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Split 4.1 & 4.2 data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tweetsData_reduced['airline_sentiment']\n",
    "labels = labels.replace(\"neutral\", \"0\").replace(\"negative\", \"-1\").replace(\"positive\", \"1\")\n",
    "labels = labels.astype('int')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(tweetsData_reduced_features_1, labels, test_size=0.3, random_state=1)\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(tweetsData_reduced_features_2, labels, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Evaluate model - Use LinearSVC + 4.1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=1, tol=0.0001,\n",
      "          verbose=0)\n",
      "0.7574453551912568\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "svc1 = LinearSVC(random_state=1)\n",
    "svc1 = svc1.fit(X_train_1, y_train_1)\n",
    "\n",
    "print(svc1)\n",
    "print(np.mean(cross_val_score(svc1, tweetsData_reduced_features_1, labels, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7789162112932605"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the result for test data using the model built above.\n",
    "\n",
    "result1 = svc1.predict(X_test_1)\n",
    "svc1.score(X_test_1, y_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2396  252   93]\n",
      " [ 301  542   93]\n",
      " [ 135   97  483]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat_1 = confusion_matrix(y_test_1, result1)\n",
    "print(conf_mat_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Evaluate model - Use LinearSVC + 4.2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=1, tol=0.0001,\n",
      "          verbose=0)\n",
      "0.7661202185792348\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "svc2 = LinearSVC(random_state=1)\n",
    "svc2 = svc2.fit(X_train_2, y_train_2)\n",
    "\n",
    "print(svc2)\n",
    "print(np.mean(cross_val_score(svc2, tweetsData_reduced_features_2, labels, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7871129326047359"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the result for test data using the model built above.\n",
    "\n",
    "result2 = svc2.predict(X_test_2)\n",
    "svc2.score(X_test_2, y_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2473  198   70]\n",
      " [ 341  520   75]\n",
      " [ 150  101  464]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat_2 = confusion_matrix(y_test_2, result2)\n",
    "print(conf_mat_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Evaluate model - Use H2O + 4.1 & 4.2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests\n",
    "#!pip install tabulate\n",
    "#!pip install \"colorama>=0.3.8\"\n",
    "#!pip install future\n",
    "#!pip uninstall h2o\n",
    "#!pip install -f http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For automated model selection\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; Java HotSpot(TM) 64-Bit Server VM (build 12.0.1+12, mixed mode, sharing)\n",
      "  Starting server from C:\\Anaconda3\\lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: C:\\Users\\tanvu\\AppData\\Local\\Temp\\tmpvk94qo0w\n",
      "  JVM stdout: C:\\Users\\tanvu\\AppData\\Local\\Temp\\tmpvk94qo0w\\h2o_tanvu_started_from_python.out\n",
      "  JVM stderr: C:\\Users\\tanvu\\AppData\\Local\\Temp\\tmpvk94qo0w\\h2o_tanvu_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Asia/Kuala_Lumpur</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.30.0.3</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>8 days </td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_tanvu_bo5yvl</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>24 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>6</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>6</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O_API_Extensions:</td>\n",
       "<td>Amazon S3, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.7.4 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------------------------------\n",
       "H2O_cluster_uptime:         02 secs\n",
       "H2O_cluster_timezone:       Asia/Kuala_Lumpur\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.30.0.3\n",
       "H2O_cluster_version_age:    8 days\n",
       "H2O_cluster_name:           H2O_from_python_tanvu_bo5yvl\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    24 Gb\n",
       "H2O_cluster_total_cores:    6\n",
       "H2O_cluster_allowed_cores:  6\n",
       "H2O_cluster_status:         accepting new members, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "H2O_API_Extensions:         Amazon S3, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python_version:             3.7.4 final\n",
       "--------------------------  ---------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start H2O cluster\n",
    "h2o.init(max_mem_size = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "AutoML progress: |\n",
      "22:15:55.505: AutoML: XGBoost is not available; skipping it.\n",
      "\n",
      "████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "train_1 = np.column_stack((X_train_1, y_train_1))\n",
    "test_1 = np.column_stack((X_test_1, y_test_1))\n",
    "\n",
    "# Convert pandas tables into H2O tables\n",
    "h2o_train = h2o.H2OFrame(train_1)\n",
    "h2o_test = h2o.H2OFrame(test_1)\n",
    "\n",
    "h2o_train[1000] = h2o_train[1000].asfactor()\n",
    "h2o_test[1000] = h2o_test[1000].asfactor()\n",
    "\n",
    "h2o_model = H2OAutoML(max_models = 20, max_runtime_secs = 1800, seed = 1)\n",
    "h2o_model.train(x = list(range(1000)), \n",
    "                y = 1000, \n",
    "                training_frame = h2o_train,  \n",
    "                leaderboard_frame = h2o_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>model_id                                           </th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DeepLearning_1_AutoML_20200521_221555              </td><td style=\"text-align: right;\">              0.341929</td><td style=\"text-align: right;\"> 1.09001 </td><td style=\"text-align: right;\">0.478358</td><td style=\"text-align: right;\">0.228827</td></tr>\n",
       "<tr><td>DeepLearning_grid__1_AutoML_20200521_221555_model_1</td><td style=\"text-align: right;\">              0.352857</td><td style=\"text-align: right;\"> 1.89441 </td><td style=\"text-align: right;\">0.48942 </td><td style=\"text-align: right;\">0.239532</td></tr>\n",
       "<tr><td>GBM_grid__1_AutoML_20200521_221555_model_1         </td><td style=\"text-align: right;\">              0.367098</td><td style=\"text-align: right;\"> 0.598104</td><td style=\"text-align: right;\">0.447953</td><td style=\"text-align: right;\">0.200662</td></tr>\n",
       "<tr><td>DeepLearning_grid__2_AutoML_20200521_221555_model_1</td><td style=\"text-align: right;\">              0.418003</td><td style=\"text-align: right;\"> 0.650591</td><td style=\"text-align: right;\">0.453851</td><td style=\"text-align: right;\">0.205981</td></tr>\n",
       "<tr><td>GBM_4_AutoML_20200521_221555                       </td><td style=\"text-align: right;\">              0.423221</td><td style=\"text-align: right;\"> 0.809137</td><td style=\"text-align: right;\">0.548632</td><td style=\"text-align: right;\">0.300997</td></tr>\n",
       "<tr><td>GBM_2_AutoML_20200521_221555                       </td><td style=\"text-align: right;\">              0.426486</td><td style=\"text-align: right;\"> 0.730861</td><td style=\"text-align: right;\">0.510498</td><td style=\"text-align: right;\">0.260608</td></tr>\n",
       "<tr><td>GBM_grid__1_AutoML_20200521_221555_model_2         </td><td style=\"text-align: right;\">              0.426687</td><td style=\"text-align: right;\"> 0.755823</td><td style=\"text-align: right;\">0.521551</td><td style=\"text-align: right;\">0.272016</td></tr>\n",
       "<tr><td>GBM_3_AutoML_20200521_221555                       </td><td style=\"text-align: right;\">              0.427254</td><td style=\"text-align: right;\"> 0.754689</td><td style=\"text-align: right;\">0.522226</td><td style=\"text-align: right;\">0.27272 </td></tr>\n",
       "<tr><td>GBM_5_AutoML_20200521_221555                       </td><td style=\"text-align: right;\">              0.429603</td><td style=\"text-align: right;\"> 0.775607</td><td style=\"text-align: right;\">0.532899</td><td style=\"text-align: right;\">0.283981</td></tr>\n",
       "<tr><td>GBM_1_AutoML_20200521_221555                       </td><td style=\"text-align: right;\">              0.436896</td><td style=\"text-align: right;\"> 0.727036</td><td style=\"text-align: right;\">0.508376</td><td style=\"text-align: right;\">0.258446</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2o_model.leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplearning prediction progress: |███████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "predictions = h2o_model.leader.predict(h2o_test)\n",
    "predictions = predictions.as_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predictions</th>\n",
       "      <th>-1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>-1</td>\n",
       "      <td>2286</td>\n",
       "      <td>311</td>\n",
       "      <td>144</td>\n",
       "      <td>2741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>341</td>\n",
       "      <td>486</td>\n",
       "      <td>109</td>\n",
       "      <td>936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "      <td>119</td>\n",
       "      <td>444</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total</td>\n",
       "      <td>2779</td>\n",
       "      <td>916</td>\n",
       "      <td>697</td>\n",
       "      <td>4392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predictions          -1    0    1  Total\n",
       "airline_sentiment                       \n",
       "-1                 2286  311  144  2741 \n",
       "0                  341   486  109  936  \n",
       "1                  152   119  444  715  \n",
       "Total              2779  916  697  4392 "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare predictions with actuals\n",
    "pd.crosstab(y_test_1.reset_index(drop=True), predictions['predict'], colnames = ['Predictions'], margins=True, margins_name=\"Total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73224043715847"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test_1.reset_index(drop=True), predictions['predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "AutoML progress: |\n",
      "22:46:09.271: AutoML: XGBoost is not available; skipping it.\n",
      "\n",
      "████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "train_2 = np.column_stack((X_train_2, y_train_2))\n",
    "test_2 = np.column_stack((X_test_2, y_test_2))\n",
    "\n",
    "# Convert pandas tables into H2O tables\n",
    "h2o_train_2 = h2o.H2OFrame(train_2)\n",
    "h2o_test_2 = h2o.H2OFrame(test_2)\n",
    "\n",
    "h2o_train_2[1000] = h2o_train_2[1000].asfactor()\n",
    "h2o_test_2[1000] = h2o_test_2[1000].asfactor()\n",
    "\n",
    "h2o_model_2 = H2OAutoML(max_models = 20, max_runtime_secs = 1800, seed = 1)\n",
    "h2o_model_2.train(x = list(range(1000)), \n",
    "                y = 1000, \n",
    "                training_frame = h2o_train_2,  \n",
    "                leaderboard_frame = h2o_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>model_id                                           </th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_AutoML_20200521_224609</td><td style=\"text-align: right;\">              0.335121</td><td style=\"text-align: right;\"> 0.574048</td><td style=\"text-align: right;\">0.432644</td><td style=\"text-align: right;\">0.187181</td></tr>\n",
       "<tr><td>DeepLearning_1_AutoML_20200521_224609              </td><td style=\"text-align: right;\">              0.33814 </td><td style=\"text-align: right;\"> 1.12791 </td><td style=\"text-align: right;\">0.478508</td><td style=\"text-align: right;\">0.22897 </td></tr>\n",
       "<tr><td>DeepLearning_grid__1_AutoML_20200521_224609_model_1</td><td style=\"text-align: right;\">              0.345546</td><td style=\"text-align: right;\"> 1.76519 </td><td style=\"text-align: right;\">0.48572 </td><td style=\"text-align: right;\">0.235924</td></tr>\n",
       "<tr><td>GBM_grid__1_AutoML_20200521_224609_model_1         </td><td style=\"text-align: right;\">              0.38209 </td><td style=\"text-align: right;\"> 0.611784</td><td style=\"text-align: right;\">0.454529</td><td style=\"text-align: right;\">0.206596</td></tr>\n",
       "<tr><td>DeepLearning_grid__3_AutoML_20200521_224609_model_1</td><td style=\"text-align: right;\">              0.385412</td><td style=\"text-align: right;\"> 0.860154</td><td style=\"text-align: right;\">0.503181</td><td style=\"text-align: right;\">0.253191</td></tr>\n",
       "<tr><td>DeepLearning_grid__2_AutoML_20200521_224609_model_1</td><td style=\"text-align: right;\">              0.425665</td><td style=\"text-align: right;\"> 0.665526</td><td style=\"text-align: right;\">0.45962 </td><td style=\"text-align: right;\">0.211251</td></tr>\n",
       "<tr><td>GBM_2_AutoML_20200521_224609                       </td><td style=\"text-align: right;\">              0.426436</td><td style=\"text-align: right;\"> 0.719101</td><td style=\"text-align: right;\">0.504586</td><td style=\"text-align: right;\">0.254607</td></tr>\n",
       "<tr><td>GBM_3_AutoML_20200521_224609                       </td><td style=\"text-align: right;\">              0.429936</td><td style=\"text-align: right;\"> 0.753945</td><td style=\"text-align: right;\">0.521706</td><td style=\"text-align: right;\">0.272177</td></tr>\n",
       "<tr><td>GBM_4_AutoML_20200521_224609                       </td><td style=\"text-align: right;\">              0.432245</td><td style=\"text-align: right;\"> 0.784857</td><td style=\"text-align: right;\">0.537003</td><td style=\"text-align: right;\">0.288372</td></tr>\n",
       "<tr><td>GBM_1_AutoML_20200521_224609                       </td><td style=\"text-align: right;\">              0.432814</td><td style=\"text-align: right;\"> 0.713927</td><td style=\"text-align: right;\">0.501767</td><td style=\"text-align: right;\">0.25177 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2o_model_2.leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "predictions_2 = h2o_model_2.leader.predict(h2o_test_2)\n",
    "predictions_2 = predictions_2.as_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predictions</th>\n",
       "      <th>-1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>-1</td>\n",
       "      <td>2522</td>\n",
       "      <td>153</td>\n",
       "      <td>66</td>\n",
       "      <td>2741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>423</td>\n",
       "      <td>435</td>\n",
       "      <td>78</td>\n",
       "      <td>936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>181</td>\n",
       "      <td>98</td>\n",
       "      <td>436</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total</td>\n",
       "      <td>3126</td>\n",
       "      <td>686</td>\n",
       "      <td>580</td>\n",
       "      <td>4392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predictions          -1    0    1  Total\n",
       "airline_sentiment                       \n",
       "-1                 2522  153  66   2741 \n",
       "0                  423   435  78   936  \n",
       "1                  181   98   436  715  \n",
       "Total              3126  686  580  4392 "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare predictions with actuals\n",
    "pd.crosstab(y_test_2.reset_index(drop=True), predictions_2['predict'], colnames = ['Predictions'], margins=True, margins_name=\"Total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7725409836065574"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test_2.reset_index(drop=True), predictions_2['predict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Vectorization - Use CountVectorizer + 2000 most-frequently used features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 2000)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorization (Convert text data to numbers).\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "vectorizer = CountVectorizer()                # Keep only 1000 features as number of features will increase the processing time.\n",
    "tweetsData_reduced_features_3 = vectorizer.fit_transform(tweetsData_reduced['text'])\n",
    "svd1 = TruncatedSVD(n_components=2000, random_state=1)\n",
    "tweetsData_reduced_features_3 = svd1.fit_transform(tweetsData_reduced_features_3) \n",
    "tweetsData_reduced_features_3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Vectorization - Use TfidfVectorizer + 2000 most-frequently used features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 2000)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using TfidfVectorizer to convert text data to numbers.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tweetsData_reduced_features_4 = vectorizer.fit_transform(tweetsData_reduced['text'])\n",
    "svd2 = TruncatedSVD(n_components=2000, random_state=1)\n",
    "tweetsData_reduced_features_4 = svd2.fit_transform(tweetsData_reduced_features_4) \n",
    "tweetsData_reduced_features_4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Split 6.1 & 6.2 data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tweetsData_reduced['airline_sentiment']\n",
    "labels = labels.replace(\"neutral\", \"0\").replace(\"negative\", \"-1\").replace(\"positive\", \"1\")\n",
    "labels = labels.astype('int')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(tweetsData_reduced_features_3, labels, test_size=0.3, random_state=1)\n",
    "X_train_4, X_test_4, y_train_4, y_test_4 = train_test_split(tweetsData_reduced_features_4, labels, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Evaluate model - Use LinearSVC + 6.1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=1, tol=0.0001,\n",
      "          verbose=0)\n",
      "0.7476092896174864\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "svc3 = LinearSVC(random_state=1)\n",
    "svc3 = svc1.fit(X_train_3, y_train_3)\n",
    "\n",
    "print(svc3)\n",
    "print(np.mean(cross_val_score(svc3, tweetsData_reduced_features_3, labels, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7668488160291439"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the result for test data using the model built above.\n",
    "\n",
    "result3 = svc3.predict(X_test_3)\n",
    "svc3.score(X_test_3, y_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2363  260  118]\n",
      " [ 298  517  121]\n",
      " [ 132   95  488]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat_3 = confusion_matrix(y_test_3, result3)\n",
    "print(conf_mat_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Evaluate model -  Use LinearSVC + 6.2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=1, tol=0.0001,\n",
      "          verbose=0)\n",
      "0.7594945355191257\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "svc4 = LinearSVC(random_state=1)\n",
    "svc4 = svc4.fit(X_train_4, y_train_4)\n",
    "\n",
    "print(svc4)\n",
    "print(np.mean(cross_val_score(svc4, tweetsData_reduced_features_4, labels, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7841530054644809"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the result for test data using the model built above.\n",
    "\n",
    "result4 = svc4.predict(X_test_4)\n",
    "svc4.score(X_test_4, y_test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2480  189   72]\n",
      " [ 347  500   89]\n",
      " [ 153   98  464]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat_4 = confusion_matrix(y_test_4, result4)\n",
    "print(conf_mat_4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
